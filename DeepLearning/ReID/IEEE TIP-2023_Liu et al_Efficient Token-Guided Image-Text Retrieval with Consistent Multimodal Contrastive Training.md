# Efficient Token-Guided Image-Text Retrieval withConsistent Multimodal Contrastive Training

> paper: https://arxiv.org/pdf/2306.08789.pdf 
>
> code: https://github.com/LCFractal/TGDT

## 1. 论文核心思想

- 整合了粗粒度与细粒度检索，利用了二者的优点
- 新的训练目标 ：Consistent Multimodal Contrastive (CMC) loss  ，确保模态内和模态间语义一致性
- 基于混合全局和局部的跨模态相似性两阶段推理方法（先全局后局部）
- 效果：检索精度高，推理时间小

### 1.1 前置知识

>  Image-Text Retrieval 
>
> 图像-文本检索涉及**根据文本查询从数据集中搜索和检索相关图像**，反之亦然。这个过程依赖于对图像和文本之间语义内容的理解和匹配。从本质上讲，它需要一个系统来准确解释视觉和文本数据中的上下文和细节，以查找对应关系，从而实现增强搜索引擎、协助内容发现和促进多媒体数据库管理等应用。



****

![算法核心流程](https://raw.githubusercontent.com/thisisbaiy/PicGo/main/image-20240215162432429.png)

上图展示了如何整合图像和文本数据来进行全局和局部检索任务。

1. **图像编码器（Image Encoder）**：
   - 使用Faster R-CNN作为基础模型提取图像特征。
   - 图像被划分为多个区域，每个区域由一个边界框（BoundingBox, BBox）表示，并且为每个区域提取特征向量（V1, V2, ..., Vr）。
   - 这些特征向量接着被输入到一个Transformer编码器中，以学习全局（I_GLO）和局部（instance features）的图像表示。

2. **文本编码器（Text Encoder）**：
   - 文本（例如一个描述图像内容的句子）首先通过BERT模型进行处理。
   - BERT模型为输入文本的每个token生成一个特征向量（t0, t1, ..., tN）。
   - 类似于图像编码器，这些文本特征向量也被输入到另一个Transformer编码器中，以学习全局（T_GLO）和局部（token features）的文本表示。

3. **全局检索（Global Retrieval）**：
   - 利用图像和文本的全局特征来计算一个全局相似性矩阵（Global Similarity Matrix）。
   - 这个矩阵衡量了整个图像与整个文本描述之间的匹配程度。
   - 全局图像-文本匹配（Global Image-Text Matching）用于推断（Inference Sg(I,T)）整体上图像和文本描述是否对应。

4. **一致的多模态对比损失（Consistent Multimodal Contrastive Loss）**：
   - 为了训练模型，采用一致性多模态对比损失确保图像和文本的全局表示能够紧密匹配。
   - 这有助于优化模型，使其能够更好地匹配相关的图像-文本对，并区分不相关的对。

5. **局部检索（Local Retrieval）**：
   - 利用图像的局部（instance）特征和文本的token特征来计算局部相似性矩阵（Local Similarity Matrix）。
   - 与全局检索不同，局部检索关注于图像特定区域和文本中特定token之间的对应关系。
   - Token感知对齐（Token-Aware Alignment）用于推断（Inference Sl(I,T)）局部元素之间的匹配程度。

6. **两阶段推理方法（Two-Stage Inference Method）**：
   - 这可能指的是模型采用两个步骤来综合全局和局部信息，以提高检索任务的准确性。
   - 第一阶段可能关注于全局匹配，而第二阶段则细化到局部或token级别的匹配。



### 1.2 图像处理

- Faster R-CNN 产生

	1. 图片中实例的位置及其特征（局部特征）
	1. 图片全局特征

- 通过transformer encoder  产生cross-modal representations  

### 1.3 文本处理

- BERT产生单词以及句子级别的特征
- 通过另一个 transformer encoder 产生linguistic representations  

### 1.4 全局检索和局部检索

- 全局检索匹配**整个图像的特征和句子级别的特征**

- 局部检索在token-level alignment后获得**跨模态相似性**

- > Token-level alignment 意味着在多模态任务中，对于每个模态（例如文本和图像），模型在处理它们时会保持标记级别的对齐。换句话说，模型会尝试将文本中的每个单词或图像中的每个区域与其他模态中的相应部分进行对齐，以便模型能够理解它们之间的关联。
  >
  > 例如，在图像描述任务中，模型需要对图像中的每个区域与文本描述中的每个单词进行对齐。这种对齐可以帮助模型理解图像中的不同对象或场景与文本描述中的相应概念之间的对应关系。类似地，在音频描述任务中，模型需要将音频片段中的每个时间步与文本描述中的每个单词进行对齐。
  >
  > 在多模态任务中，保持标记级别的对齐非常重要，因为它可以帮助模型更好地理解不同模态之间的关联，从而更准确地执行任务，如图像描述、视觉问答或视频理解。

### 1.5 Consistent Multimodal Contrastive Training (CMC) loss 

使用Consistent Multimodal Contrastive Training (CMC) loss 同时训练两个网络（全局和局部）

![image-20240215170910709](https://raw.githubusercontent.com/thisisbaiy/PicGo/main/image-20240215170910709.png)

1. **多模态对比损失**（图a）
   - 图中表示，传统的多模态对比损失函数不能控制不同模态样本之间的距离，它只减少匹配样本之间的距离并增加不匹配样本之间的距离。在图示中，星形代表图像样本，三角形代表文本样本，配对的多模态样本被表示为相同颜色的星形和三角形。
2. **一致性多模态对比损失**（图b）
   - 提出的一致性多模态对比损失（Consistent Multimodal Contrastive Loss，CMC）可以确保无论是相同模态还是不同模态的样本之间，距离都是一致的。这意味着该损失函数旨在保持所有样本（无论是图像对图像、文本对文本，还是图像对文本）之间的距离一致性。

**传统方法Multimodal Contrastive Loss ：**

![image-20240215171400500](https://raw.githubusercontent.com/thisisbaiy/PicGo/main/image-20240215171400500.png)

也就是通过找到一个锚点图片的最难负样本和相应的文本，通过找到一个锚点文本的最难负样本和相应的图片

**缺点：**对于同一模态的样本缺乏限制

**改进Consistent Multimodal Contrastive Loss  ：**

![image-20240215171531345](https://raw.githubusercontent.com/thisisbaiy/PicGo/main/image-20240215171531345.png)

增加了模态内约束。它旨在确保在同一模态（图像与图像或文本与文本）中，不匹配的对也在嵌入空间中分离。变量σ充当松弛变量，以灵活地控制来自不同模态的样本距离之间的间隙，允许一些不一致以适应距离的自然变化。

> ### $L_r$（常规对比损失）
>
> 这个公式计算了一个图像 $I$ 和一个正文本 $T$ 之间的相似度得分 $S(I, T)$，以及与负样本 $T^-$ 和 $I^-$ 之间的得分。目标是最大化正样本对的相似度得分，并最小化负样本对的得分。
>
> - 第一部分 $\max(0, \delta - S(I, T) + S(I, T^-))$：这是图像检索任务的损失，它惩罚那些相似度 $S(I, T^-)$（图像与负文本对）比 $S(I, T)$（图像与正文本对）加上一个边界 $\delta$ 更高的情况。
> - 第二部分 $\max(0, \delta - S(I, T) + S(I^-, T))$：这是文本检索任务的损失，它惩罚那些相似度 $S(I^-, T)$（负图像与文本对）比 $S(I, T)$（正图像与文本对）加上一个边界 $\delta$ 更高的情况。
>
> ### $L_a$（自适应对比损失）
>
> 这个公式也是用于计算图像和文本之间的相似度得分，但它引入了一个额外的自适应边界 $\sigma$。
>
> - 第一部分 $\max(0, S(I, I^-) - S(T, T^-) - \sigma)$：这考虑了图像之间的相似度得分 $S(I, I^-)$ 和文本之间得分 $S(T, T^-)$ 的差异，如果图像相似度得分高于文本相似度得分超过 $\sigma$，则会产生损失。
> - 第二部分 $\max(0, S(I, I^-) - S(T, T^-) - \sigma)$：这与第一部分类似，但是对另一组负样本 $I_v$ 和 $T_v$ 进行操作。
>
> 在ContrastiveLoss类中，使用compute_contrastive_loss方法来计算类似$L_r$的损失，目标是确保正样本对之间的相似度得分高于负样本对。而在AlignmentContrastiveLoss类中，可能会使用类似$L_a$的计算方法，但更专注于图像和文本之间的对齐。通过这些损失函数，模型可以学习将相似的图像和文本对靠近，同时将不相似的对分开，这是多模态表示学习的关键部分。

**CMC Loss**

![image-20240215174126774](https://raw.githubusercontent.com/thisisbaiy/PicGo/main/image-20240215174126774.png)

一方面，Lr控制样本之间的距离。另一方面，La保证了匹配样本之间距离的一致性。因此，结合两种损失可以保证多模态样本之间的**局部和全局相似度的一致性。**

### 1.6 Two-Stage Inference Method  

> inference
>
> 在深度学习中，**"inference"（推断）**是指使用**已经训练好的模型**来对新的、未见过的数据进行预测或分类的过程。在推断阶段，模型已经完成了训练过程，参数已经被学习，并且模型已经具备了对数据进行预测或分类的能力。
>
> Multitask learning
>
> 多任务学习是一种机器学习范式，其中模型被训练以**同时执行多个任务**。与为每个任务训练单独的模型不同，多任务学习利用**跨任务共享的信息来提高每个单独任务的性能**。当任务相关或共享潜在结构或特征时，这种方法尤其有益。

#### 1.6.1 Training

最终训练损失函数：

![image-20240215174946765](https://raw.githubusercontent.com/thisisbaiy/PicGo/main/image-20240215174946765.png)

表示总损失是**全局和局部相似性的 CMC 损失之和**。这种组合损失函数允许模型在**端到端**可训练网络中跨两种模态（图像和文本）共同学习全局和局部表示。该方法旨在利用多任务学习来约束参数空间，与单独训练它们相比，在这两个任务上都能获得更好的表示和更高的性能。

#### 1.6.2 Inference  

为了平衡精度和速度，该文提出了一种**两阶段推理过程**。

- 在第一阶段，使用**全局检索来快速缩小候选样本的范围**。
- 在第二阶段，结合了**全局和局部信息的混合相似性**用于对这些候选者进行重新排名，以获得更准确的最终结果。

这种双层方法允许快速进行初始过滤，然后进行更精确的选择，从而优化检索任务的速度和准确性。

## 2. 代码

### 2.1 训练

1. 加载相关配置文件与预设参数

> - 指定log文件与对象

2. 加载数据集

```python
    train_loader, val_loader = data.get_loaders(config, opt.workers)
```

> 1. `collate_fn = Collate(config)` 这行代码为了提供一个定制的批处理函数，以确保在训练和验证过程中能够正确地处理数据。
> 2. 通过 `get_paths` 函数获取数据集的根路径和相关信息，包括图像路径、注释文件路径以及数据集的拆分情况等。
> 3. 使用 `get_transform` 函数根据数据集的名称和拆分情况获取相应的数据转换（数据预处理）操作。这些转换操作通常包括将图像转换为 PyTorch 张量、归一化等操作。
> 4. 调用 `get_loader_single` 函数来创建训练和验证数据加载器。该函数用于构建单个数据集的数据加载器，并设置了一些参数，如批量大小、是否随机打乱数据等。
> 5. 返回创建好的**训练和验证数据加载器**。

3. `get_model(config)`函数是一个简单的函数，用于获取模型。创建了一个 `BASELINE` 类的实例并返回该实例。

> - `BASELINE` 的模型类是一个 PyTorch 模型，用于**图像和文本的联合表示学习任务**。
>
> 这个模型由两个主要组件组成：
>
> 1. `JointTextImageTransformerEncoder` 类：该类定义了一个联合的文本-图像编码器，它将输入的图像和文本进行编码，并输出它们的联合表示。这个编码器包含了一个文本编码器（`EncoderText`），一个图像编码器（`EncoderImage`），以及一些转换器编码层（`TransformerEncoderLayer`）用于处理编码后的特征。该类的 `forward` 方法定义了模型的前向传播逻辑，包括将输入的图像和文本进行编码，并将它们的特征通过一系列的转换器编码层进行处理，最后输出得到联合的图像和文本表示。
> 2. `BASELINE` 类：该类是模型的主类，它包含了 `JointTextImageTransformerEncoder` 类的一个实例，并定义了模型的前向传播逻辑和损失函数。该类的 `forward` 方法将输入的图像和文本传递给 `JointTextImageTransformerEncoder` 类进行编码，并根据任务类型计算损失。
>
> 此外，模型还包括一些其他的辅助函数和变量，用于初始化模型参数、设置训练模式等。

---

> - **`models.text`**
>
> 定义了两个文本编码器：`EncoderTextGRU` 和 `EncoderTextBERT`。
>
> 1. `EncoderTextGRU` 类是一个基于GRU的文本编码器。它将输入的文本序列中的单词索引转换为单词嵌入，并通过一个或多个GRU层对单词嵌入进行编码。编码后的输出是文本的表示，其维度为`[batch_size, hidden_size]`。
> 2. `EncoderTextBERT` 类是基于BERT的文本编码器。它可以直接使用预训练的BERT模型来对输入的文本序列进行编码，也可以选择在BERT模型之后添加一个或多个Transformer编码层以进一步处理文本特征。最终输出的文本表示与 `EncoderTextGRU` 类似，也是一个维度为`[batch_size, hidden_size]`的张量。

---

> - **`models.visual`**
>
> 定义了几个图像编码器，根据配置文件中的参数选择合适的编码器类型。
>
> 1. `EncoderImageFull`: 它可以选择使用预训练的CNN模型（如VGG或ResNet）提取图像特征，并将这些特征映射到固定维度的向量空间。它还支持使用Transformer网络对图像特征进行进一步处理。比如它可以选择在CNN之后添加一个Transformer网络层。最终输出的图像表示是一个维度为`[batch_size, embed_size]`的张量。
> 2. `EncoderImagePrecomp`: 预先提取的图像特征编码器。它假设输入已经是预先计算好的图像特征，并且直接将这些特征映射到固定维度的向量空间。这种编码器通常用于使用预先提取的图像特征的情况，例如在某些数据集上已经有提前计算好的图像特征。
> 3. `TransformerPostProcessing`: 基于Transformer的图像编码器，它使用Transformer网络对输入的视觉特征进行处理。它支持在Transformer之后添加池化层以及线性投影层，以便将输出的特征映射到固定维度的向量空间。最终输出的图像表示也是一个维度为`[batch_size, embed_size]`的张量。
>
> ---
>
> 1. **EncoderImage**:
>    - 这是一个封装类，根据配置决定使用预先计算的图像特征编码器`EncoderImagePrecomp`，还是实时计算图像特征的编码器`EncoderImageFull`。
>    - 如果图像特征已预先提取，则使用`EncoderImagePrecomp`，否则根据配置中指定的模型类型（如CNN或Transformer）使用`EncoderImageFull`或其他自定义的图像处理模型（如`TransformerPostProcessing`或`GCNVisualReasoning`）。
> 2. **TransformerPostProcessing**:
>    - 这个类使用Transformer编码器对图像特征进行后处理。
>    - 它接收一个特征维度和嵌入尺寸，创建一个或多个Transformer编码器层，并可选择添加位置编码。
>    - 位置编码可以与图像的边界框信息融合，然后再通过Transformer编码器。
>    - 输出是处理后的图像特征，可以选择通过门控机制进行聚合，以得到最终的图像表示。
> 3. **GCNVisualReasoning**:
>    - 这个类实现了图像特征的图卷积网络（GCN）推理。
>    - 它首先将图像特征投影到嵌入空间，然后通过一系列GCN层来提炼特征，最后使用GRU对这些特征进行序列建模。
>    - 输出是经过图卷积网络推理和GRU处理后的图像特征。
> 4. **EncoderImageFull**:
>    - 这是一个完整的图像编码器，可以选择加载预训练的CNN模型（如VGG或ResNet）。
>    - 可以对这些CNN模型进行微调，将最后的全连接层替换为新的线性层以匹配嵌入尺寸。
>    - 这个类还可以选择使用Transformer对空间特征进行后处理，输出经过归一化和投影后的图像嵌入。
> 5. **EncoderImagePrecomp**:
>    - 这个类适用于已经预计算好的图像特征。
>    - 它包含一个线性层，将预计算的图像特征投影到嵌入空间，并执行归一化和绝对值操作（如果需要）。

---

> - 得到特征后根据相似度计算损失
>
> **对齐对比损失（Alignment Contrastive Loss）也就是CMC Loss?**
>
> 对齐对比损失旨在优化模型以确保相对应的图像和文本嵌入彼此靠近，同时使不相关的嵌入远离。在这种损失函数中，对齐的概念不仅仅是简单的匹配。它可能涉及到更复杂的关系，比如序列中的不同元素（图像中的对象或文本中的单词）之间的对齐。这种方法试图捕获图像和文本之间更细粒度的关系，而不是仅仅基于整体的图像-文本相似度。
>
> **标准对比损失（Standard Contrastive Loss）**
>
> 标准对比损失，通常简称为对比损失，是一种更直观的方法，目的是缩小正样本对（即相关的图像和文本对）之间的距离，同时扩大负样本对（不相关的图像和文本对）之间的距离。这种方法通常涉及计算一个相似度矩阵，其中每个元素代表一个图像-文本对的相似度得分，然后通过比较正样本对和负样本对的得分来计算损失。

4. 设置Adam优化器

> 根据模型的不同参数部分设置不同的学习率来初始化一个优化器。

5. 设置学习率调度器

> 根据配置设置学习率调度器，用于在训练过程中调整学习率，为了改善模型的训练和收敛性能。
>
> 过适时地降低学习率，可以减小参数更新的幅度，从而允许模型在接近最优解时进行更精细的调整。
>
> - **`StepLR`**：适用于在特定训练周期后需要明确减少学习率的情况。

6. 配置设置预热（Warmup）学习率调度器

> 预热调度器在训练开始的几个周期内逐渐增加学习率，通常用于帮助模型稳定地开始训练，特别是在从非常小的学习率开始时。这可以预防模型在训练初期的激烈波动，提高模型收敛速度和最终性能。

7. 检查点恢复或模型加载
8. load the ndcg scorer

> 归一化折损累计增益（Normalized Discounted Cumulative Gain, NDCG）

9. 训练（Baseline）

### 2.2 test

#### test_gl

> ```python
> import evaluate_utils.evaluation as evaluation
> ```

#### test

> ```python
> import evaluation
> ```

二者差别在使用不同的评估

而两个evaluation的差别主要在 `i2t` 和 `t2i`

****

图像到文本的评估，评估一个模型如何能够将图像正确地关联到对应的文本描述。

|                           | evaluate_utils.evaluation                                    | **evaluation**                                               |
| :------------------------ | :----------------------------------------------------------- | ------------------------------------------------------------ |
| i2t（从图像到文本的搜索） | 先计算全局相似度（`d_g`），然后**基于全局相似度高的前50个结果再计算局部相似度**（`d_l`），最终将两者结合用于排序。 | 直接计算                                                     |
| t2i（从文本到图像的搜索） | 计算查询文本和图像之间的全局相似度。 基于全局相似度，**选择前100个最相似的图像**进行局部相似度计算`d_l`。 | 不区分全局和局部相似度计算，所有操作基于一致的相似度计算逻辑。 直接对相似度结果进行排序，确定排名，并计算评估指标。 |

